# GDT Training

This directory contains the training pipeline for **Genie Digital Twin (GDT)**, a pan-cancer longitudinal foundation model built using the TwinWeaver framework. GDT was trained on 93,054 patients across 20 cancer indications using data from Flatiron Health and Foundation Medicine.

## Overview

GDT is a fine-tuned Llama 3.1 8B Instruct model that performs:
- **Blood biomarker forecasting**: Predicting continuous time-series values up to 13 weeks ahead
- **Clinical event prediction**: Landmark prediction for survival, progression, metastasis, and therapy switching events at horizons up to 104 weeks

The model jointly learns from multi-modal patient data including:
- Demographics and diagnoses
- Longitudinal laboratory measurements (blood biomarkers, vitals)
- Genetic mutation panels (Foundation Medicine Core, 300+ genes)
- Treatment history and drug administrations
- ECOG performance status

## Training Configuration

### Model Architecture
- **Base Model**: Llama 3.1 8B Instruct
- **Context Length**: 8,000 tokens
- **Fine-tuning Method**: Full parameter fine-tuning with FSDP (Fully Sharded Data Parallel)

### Hyperparameters
- **Learning Rate**: 1e-5
- **Optimizer**: AdamW with weight decay 0.1
- **Epochs**: 1
- **Batch Size**: 1 per GPU (8 GPUs total)
- **Gradient Clipping**: 1.0
- **Precision**: bfloat16 (pure_bf16)
- **Splits per Patient**: 10 (samples generated per line of therapy)

### Data Sampling Strategy
Training samples are generated by splitting patient trajectories at clinically relevant time points:
- Split times sampled uniformly from visits within 90 days of a new line of therapy
- 10 splits per patient per line of therapy, generating ~2.49 million training samples
- Forecasting horizon: 13 weeks for blood biomarkers
- Event prediction horizons: Uniformly sampled from 1-104 weeks

### Training Infrastructure
- **Hardware**: 8x H100 GPUs
- **Training Time**: ~7 days (~178 GPU hours)
- **Distributed Strategy**: FSDP with sharded state dict checkpointing
- **Checkpoint Interval**: Every 5,000 steps (keeps 4 most recent)

## Dataset

The model was trained on the Flatiron Health-Foundation Medicine Clinico-Genomic Database (CGDB):
- **Total Patients**: 93,054
- **Cancer Indications**: 20 (NSCLC, breast cancer, colorectal cancer, etc.)
- **Data Split**: 82,753 train / 4,991 validation / 4,999 test (patient-level split)
- **Data Resolution**: Weekly aggregation
- **Preprocessing**: 3-sigma outlier filtering, weekly temporal alignment

The data includes:
- **283 unique therapies** across all indications
- **74-90 blood biomarkers** per indication (forecasted variables)
- **300+ genetic markers** from Foundation Medicine comprehensive genomic profiling
- **4 key clinical events**: Survival, progression, metastasis, therapy switching

## Repository Structure

```
2_gdt_training/
├── README.md                          # This file
├── requirements.txt                   # Python dependencies
├── base_loader.py                     # Core data loading and tokenization logic
└── v1_llama3_finetuning_10_samples_2025_01_09/
    ├── launch_training.sh             # Training launch script
    └── load_fh_instruct_dataset_10.py # Dataset configuration (10 splits/patient)
```

## Installation

### Prerequisites
- Python 3.8+
- CUDA-compatible GPUs (training tested on 8x H100)
- Access to Flatiron Health-Foundation Medicine CGDB data

### Install Dependencies
```bash
pip install -r requirements.txt
```

### Setup Llama Recipes
The training script relies on the `finetuning.py` script from the `llama-recipes` repository. You must clone this repository into the `2_gdt_training` directory (or configure the path in `launch_training.sh`):

```bash
# From within the 2_gdt_training/ directory
git clone https://github.com/meta-llama/llama-recipes.git
cd llama-recipes
# Checkout a version compatible with the installed package (optional but recommended)
git checkout v0.0.4
cd ..
```

**Key Dependencies:**
- `llama-recipes==0.0.4.post1` (with minor bug fixes for interim FSDP saving)
- `torch>=2.0.0`
- `transformers>=4.31.0`
- `datasets>=2.14.0`
- `wandb>=0.15.0` (for experiment tracking)

## Training Pipeline

### 1. Data Preparation

The training pipeline uses the `base_loader.py` module to:
- Load JSONL files containing serialized patient trajectories
- Apply the Llama 3 chat template with custom system prompt
- Tokenize using the instruction-answer format
- Mask out prompt tokens (only compute loss on assistant responses)
- Apply length truncation to 8,000 tokens


**Data Format:**
- Input: Patient history serialized as text with chronological visits
- Output: Structured predictions for forecasting tasks and event predictions
- Tokenization: Llama 3 chat template with special tokens

### 2. Training Execution

Launch training using the provided script:
```bash
cd v1_llama3_finetuning_10_samples_2025_01_09
./launch_training.sh
```


### 3. Data Loading Details

The `base_loader.py` implements:

**Tokenization Pipeline:**
1. Construct chat messages (system, user instruction, assistant answer)
2. Apply Llama 3 chat template
3. Find assistant header position in token sequence
4. Mask all tokens before assistant response with -100
5. Validate: BOS token, EOT token, input/target alignment


## Notes and Caveats

### Modified llama-recipes
We used `llama-recipes==0.0.4.post1` with custom modifications:
- Added `splits_per_patient_therapy` parameter in `datasets.py`
- Optimized `__init__` to compute lengths in `sampler.py`
- Added interim checkpoint saving functionality

**Warning**: These modifications are not in the official llama-recipes package. Please reach out to use if you 
want to get copies of these modifications.



